{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports all the required packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import (InMemoryDataset, download_url, extract_zip,Data)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rdkit.Chem import DataStructs\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "try:\n",
    "    import rdkit\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import AllChem\n",
    "    from rdkit import rdBase\n",
    "    from rdkit.Chem.rdchem import HybridizationType\n",
    "    from rdkit import RDConfig\n",
    "    from rdkit.Chem import ChemicalFeatures\n",
    "    from rdkit.Chem.rdchem import BondType as BT\n",
    "    from rdkit.Chem import Draw\n",
    "    rdBase.DisableLog('rdApp.error')\n",
    "except ImportError:\n",
    "    rdkit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition used for regression\n",
    "\n",
    "class GCNlayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, conv_dim1, conv_dim2, conv_dim3, concat_dim, dropout):\n",
    "        super(GCNlayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.conv_dim1 = conv_dim1\n",
    "        self.conv_dim2 = conv_dim2\n",
    "        self.conv_dim3 = conv_dim3\n",
    "        self.concat_dim =  concat_dim\n",
    "        self.dropout = dropout   # Dropout Layer \n",
    "        \n",
    "        self.conv1 = GCNConv(self.n_features, self.conv_dim1)\n",
    "        self.bn1 = BatchNorm1d(self.conv_dim1)\n",
    "        self.conv2 = GCNConv(self.conv_dim1, self.conv_dim2)\n",
    "        self.bn2 = BatchNorm1d(self.conv_dim2)\n",
    "        self.conv3 = GCNConv(self.conv_dim2, self.conv_dim3)\n",
    "        self.bn3 = BatchNorm1d(self.conv_dim3)\n",
    "        self.conv4 = GCNConv(self.conv_dim3, self.concat_dim)\n",
    "        self.bn4 = BatchNorm1d(self.concat_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        # Global sum aggregrator function\n",
    "        x = global_add_pool(x, data.batch) \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "    \n",
    "class FClayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, concat_dim, pred_dim1, pred_dim2, out_dim, dropout):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.concat_dim = concat_dim\n",
    "        self.pred_dim1 = pred_dim1\n",
    "        self.pred_dim2 = pred_dim2\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.fc1 = Linear(self.concat_dim, self.pred_dim1)\n",
    "        self.bn1 = BatchNorm1d(self.pred_dim1)\n",
    "        self.fc2 = Linear(self.pred_dim1, self.pred_dim2)\n",
    "        self.fc3 = Linear(self.pred_dim2, self.out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = F.elu(self.fc1(data))\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Model definition  \n",
    "class GCN_Model_reg(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCN_Model_reg, self).__init__()\n",
    "        \n",
    "        # Convolutional Layer call\n",
    "        self.conv = GCNlayer(args.n_features,args.conv_dim1,args.conv_dim2,args.conv_dim3,args.concat_dim,args.dropout)\n",
    "\n",
    "        # Fully connected layer call\n",
    "        self.fc = FClayer(args.concat_dim,args.pred_dim1,args.pred_dim2,args.out_dim,args.dropout)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.conv(data) # Calling the convolutional layer\n",
    "        x = self.fc(x) # Calling the Fully Connected Layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch will be run on cpu\n",
    "seed=200\n",
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "np.random.seed(200)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs required by the program\n",
    "args.epoch = 400\n",
    "args.lr = 0.0001\n",
    "args.optim = 'Adam'\n",
    "args.step_size = 20\n",
    "args.gamma = 0.9\n",
    "args.dropout = 0.1\n",
    "args.n_features = 29\n",
    "dim = 512\n",
    "args.conv_dim1 = dim\n",
    "args.conv_dim2 = dim\n",
    "args.conv_dim3 = dim\n",
    "args.concat_dim = dim\n",
    "args.pred_dim1 = 64\n",
    "args.pred_dim2 = 32\n",
    "args.out_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding of all variables\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    " \n",
    "def get_intervals(l):\n",
    "    \"\"\"For list of lists, gets the cumulative products of the lengths\"\"\"\n",
    "    intervals = len(l) * [0]\n",
    "    # Initalize with 1\n",
    "    intervals[0] = 1\n",
    "    for k in range(1, len(l)):\n",
    "        intervals[k] = (len(l[k]) + 1) * intervals[k - 1]\n",
    "    return intervals\n",
    "\n",
    "def safe_index(l, e):\n",
    "    \"\"\"Gets the index of e in l, providing an index of len(l) if not found\"\"\"\n",
    "    try:\n",
    "        return l.index(e)\n",
    "    except:\n",
    "        print(\"Add value to list of length {}\".format(len(l)))\n",
    "        return len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 60, 420, 1260, 3780, 18900]\n"
     ]
    }
   ],
   "source": [
    "# Atom features\n",
    "\n",
    "possible_atom_list = ['H','C', 'O', 'F','N','Cl','P','S','Si','Br','I']  # Atomic symbol 11\n",
    "\n",
    "aromatic=[0,1] # Aromatic 1\n",
    "isring=[0,1]   # Ring 1\n",
    "possible_numH_list = [0, 1, 2, 3, 4] # Total Number of bonded hydrogen atoms possible 5\n",
    "num_bonds = [0, 1, 2, 3, 4, 5]  # Total Number of Hs a carbon can bond / Total number of bonds an atom make 6\n",
    "possible_formal_charge_list = [-4,-3, -2, -1, 0, 1, 2, 3, 4]\n",
    "\n",
    "# sp3d is removed because it doesnot vary according to the paper . Hybridization 4\n",
    "possible_hybridization_list = [Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,Chem.rdchem.HybridizationType.SP3,Chem.rdchem.HybridizationType.SP3D2]\n",
    "\n",
    "# storing all the features in a detailed list\n",
    "reference_lists = [possible_atom_list, possible_numH_list,possible_formal_charge_list, num_bonds,aromatic,isring,possible_hybridization_list]\n",
    "\n",
    "intervals = get_intervals(reference_lists)\n",
    "print(intervals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total number of atom features used here is = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the entire feature list\n",
    "def get_feature_list(atom):\n",
    "    features = 5 * [0]\n",
    "    features[0] = safe_index(possible_atom_list, atom.GetSymbol())\n",
    "    features[1] = safe_index(possible_numH_list, atom.GetTotalNumHs())\n",
    "    features[2] = safe_index(num_bonds, atom.GetImplicitValence())\n",
    "    features[3] = safe_index(possible_formal_charge_list, atom.GetFormalCharge())    \n",
    "    features[4] = safe_index(possible_hybridization_list, atom.GetHybridization())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_to_id(features, intervals):\n",
    "    \"\"\"Convert list of features into index using spacings provided in intervals\"\"\"\n",
    "    id = 0\n",
    "    for k in range(len(intervals)):\n",
    "        id += features[k] * intervals[k]\n",
    "    id = id + 1\n",
    "    return id\n",
    "\n",
    "def id_to_features(id, intervals):\n",
    "    features = 6 * [0]\n",
    "    id -= 1\n",
    "    for k in range(0, 6 - 1):\n",
    "        features[6 - k - 1] = id // intervals[6 - k - 1]\n",
    "        id -= features[6 - k - 1] * intervals[6 - k - 1]\n",
    "    features[0] = id\n",
    "    return features\n",
    "\n",
    "def atom_to_id(atom):\n",
    "    \"\"\"Return a unique id corresponding to the atom type\"\"\"\n",
    "    features = get_feature_list(atom)\n",
    "    return features_to_id(features, intervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_features(atom,bool_id_feat=False,explicit_H=False):\n",
    "    if bool_id_feat:\n",
    "        return np.array([atom_to_id(atom)])\n",
    "    else:\n",
    "        from rdkit import Chem\n",
    "        results = np.array(one_of_k_encoding_unk(atom.GetSymbol(),possible_atom_list) + \n",
    "                           one_of_k_encoding_unk(atom.GetImplicitValence(), num_bonds) + \n",
    "                           [atom.GetFormalCharge()] + \n",
    "                           one_of_k_encoding_unk(atom.GetHybridization(), possible_hybridization_list) + \n",
    "                           [atom.GetIsAromatic()]+[atom.IsInRing()])\n",
    "    if not explicit_H:\n",
    "        results = np.array(results.tolist() + one_of_k_encoding_unk(atom.GetTotalNumHs(),possible_numH_list))\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bond Features\n",
    "\n",
    "def bond_features(bond):\n",
    "    from rdkit import Chem\n",
    "    bt = bond.GetBondType()\n",
    "    bond_feats = [bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE,\n",
    "                  bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC,\n",
    "                  bond.GetIsConjugated(),bond.IsInRing()]\n",
    "    \n",
    "    # Include stereo bond features as it effects the boiling point\n",
    "    bond_feats = bond_feats + one_of_k_encoding_unk(str(bond.GetStereo()),[\"STEREONONE\", \"STEREOANY\", \"STEREOZ\", \"STEREOE\"])\n",
    "    return np.array(bond_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  atom pair of two connected molecules to pass messages\n",
    "\n",
    "def get_bond_pair(mol):\n",
    "    bonds = mol.GetBonds()\n",
    "    res = [[],[]]\n",
    "    for bond in bonds:\n",
    "        res[0] += [bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()]\n",
    "        res[1] += [bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()]\n",
    "    return res\n",
    "\n",
    "# create a graph data structure comprising of x=node_f, edge_index = bond pair info, \n",
    "def mol2vec(mol):\n",
    "    atoms = mol.GetAtoms()\n",
    "    bonds = mol.GetBonds()\n",
    "    node_f= [atom_features(atom) for atom in atoms]\n",
    "    edge_index = get_bond_pair(mol)\n",
    "    edge_attr = [bond_features(bond) for bond in bonds]\n",
    "\n",
    "    for bond in bonds:\n",
    "        edge_attr.append(bond_features(bond))\n",
    "        \n",
    "    # Graph data to be used\n",
    "    data = Data(x=torch.tensor(node_f, dtype=torch.float),edge_index=torch.tensor(edge_index, dtype=torch.long),edge_attr=torch.tensor(edge_attr,dtype=torch.float))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return mol info\n",
    "def make_mol(df):\n",
    "    mols = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        mols[Chem.MolFromSmiles(df['Smiles'].iloc[i])] = df['Tb'].iloc[i]\n",
    "    return mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset with X values and y values to be trained/tested\n",
    "def make_vec(mols):\n",
    "    X = [mol2vec(m) for m in mols.keys()]\n",
    "    for i, data in enumerate(X):\n",
    "        y = list(mols.values())[i]\n",
    "        data.y = torch.tensor([y], dtype=torch.float)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Tb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCC(O)CCC(F)(F)F</td>\n",
       "      <td>413.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCC(CC)ON(=O)=O</td>\n",
       "      <td>413.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clc1ccc(Cl)c(c1)C(=O)O</td>\n",
       "      <td>574.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC(C)Cc1ccccc1</td>\n",
       "      <td>476.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COc1ccc(cc1)NC(=O)C</td>\n",
       "      <td>608.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>BrC(F)(F)Oc1ccc(cc1)C(C)(C)COCc2cccc(Oc3ccccc3)c2</td>\n",
       "      <td>564.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>OCC(C)CC(F)(F)F</td>\n",
       "      <td>405.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5273</th>\n",
       "      <td>Clc1ccc(cc1)CC2CCC(C)(C)C2(O)CN3C=NC=N3</td>\n",
       "      <td>558.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>CCOC(=O)C(Cl)Cc1cc(c(F)cc1Cl)N2N=C(C)N(C(F)F)C2=O</td>\n",
       "      <td>625.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>CSC(=O)c1cccc2N=NSc12</td>\n",
       "      <td>540.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5276 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Smiles      Tb\n",
       "0                                      CCC(O)CCC(F)(F)F  413.80\n",
       "1                                       CCC(CC)ON(=O)=O  413.15\n",
       "2                                Clc1ccc(Cl)c(c1)C(=O)O  574.15\n",
       "3                                        NC(C)Cc1ccccc1  476.15\n",
       "4                                   COc1ccc(cc1)NC(=O)C  608.15\n",
       "...                                                 ...     ...\n",
       "5271  BrC(F)(F)Oc1ccc(cc1)C(C)(C)COCc2cccc(Oc3ccccc3)c2  564.15\n",
       "5272                                    OCC(C)CC(F)(F)F  405.65\n",
       "5273            Clc1ccc(cc1)CC2CCC(C)(C)C2(O)CN3C=NC=N3  558.15\n",
       "5274  CCOC(=O)C(Cl)Cc1cc(c(F)cc1Cl)N2N=C(C)N(C(F)F)C2=O  625.65\n",
       "5275                              CSC(=O)c1cccc2N=NSc12  540.15\n",
       "\n",
       "[5276 rows x 2 columns]"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Dataset\n",
    "dataset = pd.read_csv('../data/raw_data.csv', low_memory=False)\n",
    "dataset = pd.concat([dataset['Smiles'], dataset['Tb']], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.276000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.080471e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.212019e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.301239e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-7.437000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.041305e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.303198e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Tb\n",
       "count  5.276000e+03\n",
       "mean  -8.080471e-17\n",
       "std    1.000000e+00\n",
       "min   -4.212019e+00\n",
       "25%   -6.301239e-01\n",
       "50%   -7.437000e-03\n",
       "75%    6.041305e-01\n",
       "max    2.303198e+01"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Standardize the Boiling Point values\n",
    "mean=dataset['Tb'].mean()\n",
    "std=dataset['Tb'].std()\n",
    "dataset['Tb']=(dataset['Tb']-mean)/std\n",
    "dataset.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Checkpoint for model evaluation :\n",
    "def save_checkpoint(epoch, model, optimizer, filename):\n",
    "    state = {'Epoch': epoch,'State_dict': model.state_dict(),'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>Tb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)c1ccc(Cl)cc1</td>\n",
       "      <td>0.520735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC1CCCC(C)(C)C1=O</td>\n",
       "      <td>-0.135310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c1ccc2Sc3ccccc3Sc2c1</td>\n",
       "      <td>1.938459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N#CSCCCC</td>\n",
       "      <td>-0.051915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCCCCCCCCC(=O)OC</td>\n",
       "      <td>0.637489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>O=COC</td>\n",
       "      <td>-1.767639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>CCCCCCCCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>2.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3690</th>\n",
       "      <td>Brc1ccc(C)cc1</td>\n",
       "      <td>-0.070818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>ClN</td>\n",
       "      <td>3.661968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>COP(=O)(OC)OC</td>\n",
       "      <td>0.072623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3693 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Smiles        Tb\n",
       "0         CCOC(=O)c1ccc(Cl)cc1  0.520735\n",
       "1            CC1CCCC(C)(C)C1=O -0.135310\n",
       "2         c1ccc2Sc3ccccc3Sc2c1  1.938459\n",
       "3                     N#CSCCCC -0.051915\n",
       "4            CCCCCCCCCCC(=O)OC  0.637489\n",
       "...                        ...       ...\n",
       "3688                     O=COC -1.767639\n",
       "3689  CCCCCCCCCCCCCCCCCCCCCCCC  2.230900\n",
       "3690             Brc1ccc(C)cc1 -0.070818\n",
       "3691                       ClN  3.661968\n",
       "3692             COP(=O)(OC)OC  0.072623\n",
       "\n",
       "[3693 rows x 2 columns]"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train test split \n",
    "Train_set, Test_set = train_test_split(dataset, test_size=0.3, shuffle=True, random_state=seed)\n",
    "Train_set = Train_set.reset_index(drop=True)\n",
    "Test_set = Test_set.reset_index(drop=True)\n",
    "Train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mols = make_mol(Train_set)\n",
    "test_mols = make_mol(Test_set)\n",
    "\n",
    "train_X = make_vec(train_mols)\n",
    "test_X = make_vec(test_mols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model activation\n",
    "\n",
    "model = GCN_Model_reg(args)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_X, batch_size=100, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_X, batch_size=len(test_X), shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, optimizer, train_loader, criterion, args):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    epoch_train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = data.y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        \n",
    "        #print(outputs)\n",
    "        #print(targets)\n",
    "        outputs.require_grad = False\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += targets.size(0)\n",
    "\n",
    "        correct_dim_output=outputs.squeeze() # done to reshape the outputs and match with targets\n",
    "        loss = criterion(correct_dim_output, targets)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    print('- Loss : %.4f' % epoch_train_loss)\n",
    "    return model, epoch_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, train_loader, device, args):\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=args.lr)\n",
    "    criterion = nn.MSELoss()  # decide which loss to be used later\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "    \n",
    "    list_train_loss = list()\n",
    "    list_train_acc = list()\n",
    "    print('[Train]')\n",
    "    for epoch in range(args.epoch):\n",
    "        scheduler.step()\n",
    "        print('- Epoch :', epoch+1)\n",
    "        model, train_loss= train(model, device, optimizer, train_loader, criterion, args)\n",
    "        list_train_loss.append(train_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    save_checkpoint(epoch, model, optimizer, 'model.pt')\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      "- Epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanad\\Desktop\\Github repos\\ME_793_PROJECT\\793_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loss : 0.5784\n",
      "- Epoch : 2\n",
      "- Loss : 0.4039\n",
      "- Epoch : 3\n",
      "- Loss : 0.3547\n",
      "- Epoch : 4\n",
      "- Loss : 0.3124\n",
      "- Epoch : 5\n",
      "- Loss : 0.2933\n",
      "- Epoch : 6\n",
      "- Loss : 0.2760\n",
      "- Epoch : 7\n",
      "- Loss : 0.2715\n",
      "- Epoch : 8\n",
      "- Loss : 0.2590\n",
      "- Epoch : 9\n",
      "- Loss : 0.2514\n",
      "- Epoch : 10\n",
      "- Loss : 0.2502\n",
      "- Epoch : 11\n",
      "- Loss : 0.2434\n",
      "- Epoch : 12\n",
      "- Loss : 0.2300\n",
      "- Epoch : 13\n",
      "- Loss : 0.2237\n",
      "- Epoch : 14\n",
      "- Loss : 0.2229\n",
      "- Epoch : 15\n",
      "- Loss : 0.2228\n",
      "- Epoch : 16\n",
      "- Loss : 0.2102\n",
      "- Epoch : 17\n",
      "- Loss : 0.1995\n",
      "- Epoch : 18\n",
      "- Loss : 0.2058\n",
      "- Epoch : 19\n",
      "- Loss : 0.1943\n",
      "- Epoch : 20\n",
      "- Loss : 0.2034\n",
      "- Epoch : 21\n",
      "- Loss : 0.1050\n",
      "- Epoch : 22\n",
      "- Loss : 0.1831\n",
      "- Epoch : 23\n",
      "- Loss : 0.1805\n",
      "- Epoch : 24\n",
      "- Loss : 0.1996\n",
      "- Epoch : 25\n",
      "- Loss : 0.2030\n",
      "- Epoch : 26\n",
      "- Loss : 0.1811\n",
      "- Epoch : 27\n",
      "- Loss : 0.1759\n",
      "- Epoch : 28\n",
      "- Loss : 0.1800\n",
      "- Epoch : 29\n",
      "- Loss : 0.1870\n",
      "- Epoch : 30\n",
      "- Loss : 0.1768\n",
      "- Epoch : 31\n",
      "- Loss : 0.1720\n",
      "- Epoch : 32\n",
      "- Loss : 0.1651\n",
      "- Epoch : 33\n",
      "- Loss : 0.1840\n",
      "- Epoch : 34\n",
      "- Loss : 0.1788\n",
      "- Epoch : 35\n",
      "- Loss : 0.1605\n",
      "- Epoch : 36\n",
      "- Loss : 0.1514\n",
      "- Epoch : 37\n",
      "- Loss : 0.1615\n",
      "- Epoch : 38\n",
      "- Loss : 0.1557\n",
      "- Epoch : 39\n",
      "- Loss : 0.1841\n",
      "- Epoch : 40\n",
      "- Loss : 0.1518\n",
      "- Epoch : 41\n",
      "- Loss : 0.1551\n",
      "- Epoch : 42\n",
      "- Loss : 0.1511\n",
      "- Epoch : 43\n",
      "- Loss : 0.1328\n",
      "- Epoch : 44\n",
      "- Loss : 0.1403\n",
      "- Epoch : 45\n",
      "- Loss : 0.1361\n",
      "- Epoch : 46\n",
      "- Loss : 0.1303\n",
      "- Epoch : 47\n",
      "- Loss : 0.1311\n",
      "- Epoch : 48\n",
      "- Loss : 0.1470\n",
      "- Epoch : 49\n",
      "- Loss : 0.1487\n",
      "- Epoch : 50\n",
      "- Loss : 0.1315\n",
      "- Epoch : 51\n",
      "- Loss : 0.1472\n",
      "- Epoch : 52\n",
      "- Loss : 0.1723\n",
      "- Epoch : 53\n",
      "- Loss : 0.1333\n",
      "- Epoch : 54\n",
      "- Loss : 0.1466\n",
      "- Epoch : 55\n",
      "- Loss : 0.1263\n",
      "- Epoch : 56\n",
      "- Loss : 0.1320\n",
      "- Epoch : 57\n",
      "- Loss : 0.1346\n",
      "- Epoch : 58\n",
      "- Loss : 0.1246\n",
      "- Epoch : 59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[646], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mvars\u001b[39m(experiment(model, train_loader, device, args))\n",
      "Cell \u001b[1;32mIn[645], line 16\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(model, train_loader, device, args)\u001b[0m\n\u001b[0;32m     14\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m- Epoch :\u001b[39m\u001b[39m'\u001b[39m, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     model, train_loss\u001b[39m=\u001b[39m train(model, device, optimizer, train_loader, criterion, args)\n\u001b[0;32m     17\u001b[0m     list_train_loss\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     22\u001b[0m save_checkpoint(epoch, model, optimizer, \u001b[39m'\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[644], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, optimizer, train_loader, criterion, args)\u001b[0m\n\u001b[0;32m      7\u001b[0m targets \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     11\u001b[0m \u001b[39m#print(outputs)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#print(targets)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m outputs\u001b[39m.\u001b[39mrequire_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\ME_793_PROJECT\\793_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[625], line 74\u001b[0m, in \u001b[0;36mGCN_Model_reg.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m---> 74\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(data) \u001b[39m# Calling the convolutional layer\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x) \u001b[39m# Calling the Fully Connected Layer\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\ME_793_PROJECT\\793_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[625], line 32\u001b[0m, in \u001b[0;36mGCNlayer.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(x)\n\u001b[0;32m     31\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x, edge_index))\n\u001b[1;32m---> 32\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn4(x)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Global sum aggregrator function\u001b[39;00m\n\u001b[0;32m     35\u001b[0m x \u001b[39m=\u001b[39m global_add_pool(x, data\u001b[39m.\u001b[39mbatch) \n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\ME_793_PROJECT\\793_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\ME_793_PROJECT\\793_env\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    177\u001b[0m     bn_training,\n\u001b[0;32m    178\u001b[0m     exponential_average_factor,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    180\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kanad\\Desktop\\Github repos\\ME_793_PROJECT\\793_env\\lib\\site-packages\\torch\\nn\\functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2418\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2419\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2421\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2422\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2423\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = vars(experiment(model, train_loader, device, args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "793_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3501b5041c5233f2ce1d5f142327998ccbd12e95b6317deec921834a08ecc96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
